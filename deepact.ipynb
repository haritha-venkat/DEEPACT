{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A Generative Adversarial Network (GAN) is a type of neural network architecture used for generative modeling — learning to generate new data with the same statistics as the training data. A GAN consists of two parts:\n",
        "\n",
        "Architecture of GAN:\n",
        "Generator (G):\n",
        "\n",
        "Takes random noise as input and generates fake data (e.g., fake images).\n",
        "\n",
        "Tries to fool the Discriminator into believing the generated data is real.\n",
        "\n",
        "Discriminator (D):\n",
        "\n",
        "Takes real data and fake data as input and tries to distinguish between them.\n",
        "\n",
        "Outputs a probability (0 to 1) representing the likelihood that the input is real.\n",
        "\n",
        "These two networks are trained simultaneously in a minimax game:\n",
        "\n",
        "Generator tries to minimize the discriminator's ability to detect fake data.\n",
        "\n",
        "Discriminator tries to maximize its classification accuracy.\n",
        "\n",
        "Working Process:\n",
        "Sample random noise and generate fake data using the Generator.\n",
        "\n",
        "Feed real data and fake data to the Discriminator.\n",
        "\n",
        "Calculate the Discriminator loss using binary cross-entropy:\n",
        "\n",
        "Real data → target: 1\n",
        "\n",
        "Fake data → target: 0\n",
        "\n",
        "Update Discriminator weights to improve real/fake classification.\n",
        "\n",
        "Sample noise, generate fake data again.\n",
        "\n",
        "Pass fake data to the Discriminator.\n",
        "\n",
        "Calculate Generator loss (how well it fools the Discriminator).\n",
        "\n",
        "Update Generator weights to improve realism of generated data.\n",
        "\n"
      ],
      "metadata": {
        "id": "tkCJX53AtyU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HARITHA SHREE 212222230046"
      ],
      "metadata": {
        "id": "AZALj9VOutp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "epochs = 10\n",
        "z_dim = 100\n",
        "\n",
        "# Data loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "mnist = datasets.MNIST(root='.', train=True, transform=transform, download=True)\n",
        "dataloader = DataLoader(mnist, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 784),\n",
        "            nn.Tanh()  # Output range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Initialize models\n",
        "G = Generator(z_dim)\n",
        "D = Discriminator()\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for real_imgs, _ in dataloader:\n",
        "        real_imgs = real_imgs.view(-1, 784)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Labels\n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "\n",
        "        # ======================\n",
        "        # Train Discriminator\n",
        "        # ======================\n",
        "        z = torch.randn(batch_size, z_dim)\n",
        "        fake_imgs = G(z)\n",
        "\n",
        "        real_loss = criterion(D(real_imgs), real_labels)\n",
        "        fake_loss = criterion(D(fake_imgs.detach()), fake_labels)\n",
        "        d_loss = real_loss + fake_loss\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # ======================\n",
        "        # Train Generator\n",
        "        # ======================\n",
        "        z = torch.randn(batch_size, z_dim)\n",
        "        fake_imgs = G(z)\n",
        "        g_loss = criterion(D(fake_imgs), real_labels)  # try to fool D\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}]  D Loss: {d_loss.item():.4f}  G Loss: {g_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eba1YsRQt0Q0",
        "outputId": "f8275e51-40d2-40a8-d609-83e137d34946"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 486kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.61MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.74MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]  D Loss: 0.9080  G Loss: 0.9821\n",
            "Epoch [2/10]  D Loss: 1.4170  G Loss: 0.8058\n",
            "Epoch [3/10]  D Loss: 1.0560  G Loss: 0.9872\n",
            "Epoch [4/10]  D Loss: 1.5250  G Loss: 0.6394\n",
            "Epoch [5/10]  D Loss: 0.5656  G Loss: 1.5569\n",
            "Epoch [6/10]  D Loss: 0.9494  G Loss: 1.0714\n",
            "Epoch [7/10]  D Loss: 1.1532  G Loss: 0.9081\n",
            "Epoch [8/10]  D Loss: 1.0874  G Loss: 1.1086\n",
            "Epoch [9/10]  D Loss: 1.6146  G Loss: 0.7055\n",
            "Epoch [10/10]  D Loss: 1.0290  G Loss: 1.2013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HARITHA SHREE\n",
        "212222230046\n"
      ],
      "metadata": {
        "id": "7ZSaMCt-uwWs"
      }
    }
  ]
}